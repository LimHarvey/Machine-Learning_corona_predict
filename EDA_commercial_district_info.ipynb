{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4478604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import folium \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import io\n",
    "\n",
    "from geopandas import GeoDataFrame, GeoSeries, read_file, sjoin\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2272a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_path = './geo/geo_data'\n",
    "sur_path = './Commercial_district_information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49622722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 프로세싱에 관한 클래스\n",
    "class file_processing() :\n",
    "    def __init__(self, sur_path) :\n",
    "        # 원하는 기간 입력 받아 놓고 기본 폴더내의 파일 리스트 생성\n",
    "        file_list = os.listdir(sur_path)\n",
    "        file_list = [i for i in file_list if not os.path.isdir(sur_path+'/'+i)]\n",
    "        \n",
    "        want_year = input('년?')\n",
    "        want_start_q = input('시작 분기?')\n",
    "        want_end_q = input('끝 분기?')\n",
    "        \n",
    "        self.sur_path = sur_path\n",
    "        self.want_year = want_year\n",
    "        self.want_start_q = want_start_q\n",
    "        self.want_end_q = want_end_q\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    # 데이터를 원하는 기간 만큼만 잘라줌\n",
    "    def filter_period(self, df) :\n",
    "    \n",
    "        df = df[df['기준_년_코드'] == int(self.want_year)]\n",
    "        df = df[df['기준_분기_코드'] >= int(self.want_start_q)]\n",
    "        df = df[df['기준_분기_코드'] <= int(self.want_end_q)]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # 영역과 지점간의 매칭 dictionary 생성\n",
    "    def make_matching_dictionary(self, df_matching_data) :\n",
    "        dict_matching = {}\n",
    "        for i in range(len(df_matching_data)) :\n",
    "            dict_matching[df_matching_data.iloc[i,0]] = df_matching_data.iloc[i,1]\n",
    "        return dict_matching\n",
    "\n",
    "    # 하나의 상권에 여러 개 데이터 있을경우 하나로 합쳐줌(평균 처리)\n",
    "    def merge_same_area_data(self, a) :\n",
    "        for filename in self.file_list :\n",
    "            if filename == '서울시우리마을가게상권분석서비스(상권-추정매출)_2020.csv' :\n",
    "                self.merge_same_area_data_1(filename, a)\n",
    "            else :\n",
    "                self.merge_same_area_data_2(filename, a)\n",
    "            \n",
    "    # 상권 추정 파일의 경우 컬럼 구성이 달라서 따로 처리\n",
    "    def merge_same_area_data_1(self, file_name, df_matching_data) :  # 상권 추정매출 전용\n",
    "        print(file_name, '진입')\n",
    "        dataframe_existance = False\n",
    "        except_columns = ['기준_년_코드','기준_분기_코드','상권_구분_코드','상권_구분_코드_명','상권_코드','상권_코드_명','서비스_업종_코드','서비스_업종_코드_명','matching_상권_코드_명']\n",
    "\n",
    "        df_raw_data = pd.read_csv(self.sur_path+'/'+file_name, index_col=False, encoding='cp949')\n",
    "        df_raw_data = self.filter_period(df_raw_data)\n",
    "\n",
    "        dict_matching = self.make_matching_dictionary(df_matching_data[['raw_상권_코드_명','matching_상권명']])\n",
    "\n",
    "        df_raw_data['matching_상권_코드_명']=df_raw_data['상권_코드_명']\n",
    "        df_raw_data['matching_상권_코드_명'] = df_raw_data['matching_상권_코드_명'].apply(lambda x:dict_matching.get(x,np.nan))\n",
    "        \n",
    "        df_raw_data.to_csv('df_raw_Data.csv', encoding='cp949')\n",
    "\n",
    "        cnt=0\n",
    "        for i in  df_raw_data['matching_상권_코드_명'].unique() :                 # unique 한 'matching_상권_코드_명'별로 묶어서 처리함\n",
    "            df_tmp = df_raw_data[df_raw_data['matching_상권_코드_명']==i]\n",
    "            if cnt==20:\n",
    "                print('■',end='')\n",
    "                cnt=0\n",
    "            cnt+=1\n",
    "\n",
    "            for j in df_tmp['기준_분기_코드'].unique() :                          # unique 한 '기준_분기_코드'별로 묶어서 처리함\n",
    "                df_tmp1 = df_tmp[df_tmp['기준_분기_코드']==j]                     # matching_상권_코드_명','기준_분기_코드'묶은 데이터프레임\n",
    "\n",
    "                for k in df_tmp1['서비스_업종_코드_명'].unique() :                # unique 한 '서비스_업종_코드_명'별로 묶어서 처리함\n",
    "                    df_tmp2 = df_tmp1[df_tmp1['서비스_업종_코드_명']==k]          # matching_상권_코드_명','기준_분기_코드'묶은 데이터프레임\n",
    "                    if len(df_tmp2)>=2 :                                          # 하나의 서비스_업종_코드_명'이 두개 이상 값이 있으면\n",
    "                        list_tmp = []                                             # 정리된 데이터 프레임에 들어갈 한줄을 위한 임시 리스트\n",
    "\n",
    "                        for l in df_tmp2.columns :                                # 칼럼 별 순서대로 처리할 것임\n",
    "                            if l in except_columns :                              # 병합 제외 칼럼 처리\n",
    "                                list_tmp.append(df_tmp2.iloc[-1].loc[l])          # 해당 칼럼의 마지막 값을 임의로 정해서 병합 할거임\n",
    "                            else :                                                # 병합 제외 아닌 정상 칼럼이면\n",
    "                                list_tmp.append(float(df_tmp2.loc[:,l].sum())/len(df_tmp2))  # 칼럼별 합을 데이터 갯수로 나눠서 평균을 내어 임시 리스트에 추가\n",
    "\n",
    "                        if dataframe_existance :                                  # df_merged 데이터프레임이 이미 있으면\n",
    "                            df_list = pd.DataFrame(list_tmp).T\n",
    "                            df_list.columns = list(df_tmp2.columns)\n",
    "                            df_merged = pd.concat([df_merged,df_list])            # 거기에 그냥 붙이기\n",
    "                        else :                                                    # df_merged 가 없으면\n",
    "                            df_merged = pd.DataFrame(list_tmp).T                  # 임시 데이터로 새로 만듬\n",
    "                            df_merged.columns = list(df_tmp2.columns)\n",
    "                            dataframe_existance=True                             # 데이터 프레임 만들었다~~\n",
    "                        del list_tmp\n",
    "\n",
    "                    else :                                                        # 하나의 서비스_업종_코드_명'이 한개만 값이 있으면    \n",
    "                        if dataframe_existance :\n",
    "                            df_merged = pd.concat([df_merged,df_tmp2])\n",
    "                        else :\n",
    "\n",
    "                            df_merged = df_tmp2.copy()\n",
    "                            dataframe_existance=True\n",
    "        \n",
    "        if not os.path.exists(sur_path+'/filtered') :\n",
    "            os.mkdir(sur_path+'/filtered')\n",
    "        df_merged.to_csv(self.sur_path+'/filtered/'+file_name, encoding='cp949')\n",
    "\n",
    "    def merge_same_area_data_2(self, file_name, df_matching_data) :     # 상권 추정매출 제외 나머지 파일 처리 용\n",
    "        print(file_name, '진입')\n",
    "        dataframe_existance = False\n",
    "        except_columns = ['기준_년_코드','기준_분기_코드','상권_구분_코드','상권_구분_코드_명',' 상권_구분_코드_명','상권_코드','상권_코드_명','서비스_업종_코드','서비스_업종_코드_명','matching_상권_코드_명']\n",
    "        \n",
    "        df_raw_data = pd.read_csv(self.sur_path+'/'+file_name, index_col=False, encoding='cp949')\n",
    "        df_raw_data = self.filter_period(df_raw_data)\n",
    "\n",
    "        dict_matching = self.make_matching_dictionary(df_matching_data[['raw_상권_코드_명','matching_상권명']])\n",
    "\n",
    "        df_raw_data['matching_상권_코드_명'] = df_raw_data['상권_코드_명']\n",
    "        df_raw_data['matching_상권_코드_명'] = df_raw_data['matching_상권_코드_명'].apply(lambda x:dict_matching.get(x,np.nan))\n",
    "\n",
    "        cnt=0\n",
    "        for i in  df_raw_data['matching_상권_코드_명'].unique() :          # unique 한 'matching_상권_코드_명'별로 묶어서 처리함\n",
    "            df_tmp = df_raw_data[df_raw_data['matching_상권_코드_명']==i]\n",
    "            if cnt==20:\n",
    "                print('■',end='')\n",
    "                cnt=0\n",
    "            cnt+=1\n",
    "            for j in df_tmp['기준_분기_코드'].unique() :              # unique 한 '기준_분기_코드'별로 묶어서 처리함\n",
    "                df_tmp1 = df_tmp[df_tmp['기준_분기_코드']==j]          # matching_상권_코드_명','기준_분기_코드'묶은 데이터프레임\n",
    "\n",
    "                if len(df_tmp1)>=2 :                                   # 하나의 서비스_업종_코드_명'이 두개 이상 값이 있으면\n",
    "                    list_tmp = []                                       # 정리된 데이터 프레임에 들어갈 한줄을 위한 임시 리스트\n",
    "\n",
    "                    for k in df_tmp1.columns :                          # 칼럼 별 순서대로 처리할 것임\n",
    "                        if k in except_columns :                        # 병합 제외 칼럼 처리\n",
    "                            list_tmp.append(df_tmp1.iloc[-1].loc[k])    # 해당 칼럼의 마지막 값을 임의로 정해서 병합 할거임\n",
    "                        else :                                          # 병합 제외 아닌 정상 칼럼이면\n",
    "                            list_tmp.append(float(df_tmp1.loc[:,k].sum())/len(df_tmp1))  # 칼럼별 합을 데이터 갯수로 나눠서 평균을 내어 임시 리스트에 추가\n",
    "\n",
    "                    if dataframe_existance :                             # df_merged 데이터프레임이 이미 있으면\n",
    "                        df_list = pd.DataFrame(list_tmp).T\n",
    "                        df_list.columns = list(df_tmp1.columns)\n",
    "                        df_merged = pd.concat([df_merged,df_list])       # 거기에 그냥 붙이기\n",
    "                    else :                                              # df_merged 가 없으면\n",
    "                        df_merged = pd.DataFrame(list_tmp).T             # 임시 데이터로 새로 만듬\n",
    "                        df_merged.columns = list(df_tmp1.columns)\n",
    "                        dataframe_existance=True                        # 데이터 프레임 만들었다~~\n",
    "                    del list_tmp\n",
    "\n",
    "                else :                                                  # 하나의 서비스_업종_코드_명'이 한개만 값이 있으면  \n",
    "                    if dataframe_existance :\n",
    "                        df_merged = pd.concat([df_merged,df_tmp1])\n",
    "                    else :\n",
    "\n",
    "                        df_merged = df_tmp1.copy()\n",
    "                        dataframe_existance=True\n",
    "\n",
    "        if not os.path.exists(sur_path+'/filtered') :\n",
    "            os.mkdir(sur_path+'/filtered')\n",
    "        df_merged.to_csv(self.sur_path+'/filtered/'+file_name, encoding='cp949')\n",
    "        print(file_name+'처리 끝')\n",
    "\n",
    "# 상권 지점, 상권 지역 데이터 간의 데이터 변환 및 상권별 데이터 처리하는 클래스\n",
    "class area_info() :\n",
    "    def __init__(self, path) :\n",
    "        df = gpd.read_file(path+'/area.zip.geojson')\n",
    "        df_poly = df[['TRDAR_CD_N','geometry']]\n",
    "        df_poly.columns=['matching_상권명','geometry']\n",
    "        print(df_poly)\n",
    "        \n",
    "        self.df_poly = df_poly\n",
    "        \n",
    "    # 서울 상권 지점 데이터의 5181 좌표를 4326 좌표로 변환 \n",
    "    def load_seoul_area_data(self) : \n",
    "        df =pd.read_csv(geo_path + '/inf/서울시 우리마을가게 상권분석서비스(상권영역).csv', encoding='cp949',index_col = False)\n",
    "        df = df[['상권_구분_코드_명','상권_코드','상권_코드_명','엑스좌표_값','와이좌표_값']]\n",
    "        df.columns = ['raw_상권_구분_코드_명','raw_상권_코드','raw_상권_코드_명','엑스좌표_값','와이좌표_값']\n",
    "        df = GeoDataFrame(df, geometry=gpd.points_from_xy(df['엑스좌표_값'],df['와이좌표_값']),crs=5181)\n",
    "        df.drop(['엑스좌표_값','와이좌표_값'],axis=1,inplace=True)\n",
    "        df = df.to_crs(epsg=4326)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 서울 상권 지점 데이터 4326 데이터를 geoDataFrame 으로 변환\n",
    "    def convert_geo_to_geodataframe(self, df_geo) :\n",
    "        geo_cul_num = list(df_geo.columns).index('좌표')\n",
    "\n",
    "        for i in range(len(df_geo)) :\n",
    "            df_geo['long']=float(df_geo.iloc[i,geo_cul_num].split(',')[1])\n",
    "            df_geo['lat']=float(df_geo.iloc[i,geo_cul_num].split(',')[0])\n",
    "\n",
    "        df_geo.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # long, lat 데이터로 GeoDataFrame 변환 후 필요 없는 데이터 모두 삭제. 이름, long, lat 만 남김\n",
    "        df_geo = df_geo[['지명','long','lat']]\n",
    "        df_geo = GeoDataFrame(df_geo, geometry=gpd.points_from_xy(df_geo['long'],df_geo['lat']))\n",
    "        df_geo.drop(['long','lat'],axis=1,inplace=True)\n",
    "        return df_geo\n",
    "        \n",
    "    def is_in_area(self, point) :\n",
    "        point = point[point.geometry.type == 'Point']\n",
    "        inf_poly = gpd.sjoin(point, self.df_poly, how='inner', op='within')\n",
    "        return inf_poly\n",
    "    \n",
    "# 정리된 확진자 데이터를 불러와서 서울시 데이터만 남김\n",
    "    def load_infection_data(self) :\n",
    "        df =pd.read_csv('infection_data.csv', index_col = False)\n",
    "        df = df[['시','지명','좌표']]\n",
    "        df = df[df['시']=='서울특별시']\n",
    "        return df\n",
    "    \n",
    "\n",
    "# 정리, 통합된 상권정보에서 확진자가 나오지 않은 상권은 제외 함\n",
    "def remove_uninfection_area (path,name):\n",
    "    df_poly_286 = gpd.read_file('df_poly_286.geojson')\n",
    "    infection_area_name_list = list(df_poly_286['상권명'])\n",
    "    df = pd.read_csv(path+'/'+name,encoding='cp949')\n",
    "    for i in range(len(df)-1,-1,-1) :\n",
    "        if df.iloc[i].loc['matching_상권_코드_명'] not in infection_area_name_list :\n",
    "            df.drop(index=i, inplace=True)\n",
    "        else : \n",
    "            pass\n",
    "    df.to_csv(path+'/filtered/filtered/'+name,encoding='cp949')\n",
    "\n",
    "# 상권 정보 중에 확진자가 발견 되지 않은 지역의 데이터 행을 제거\n",
    "def remove_uninfection_area (path):\n",
    "    \n",
    "    file_list = os.listdir(path+'/filtered')\n",
    "    file_list = [i for i in file_list if not os.path.isdir(path+'/filtered/'+i)]\n",
    "\n",
    "    df_poly_286 = gpd.read_file('df_poly_286.geojson')\n",
    "    infection_area_name_list = list(df_poly_286['상권명'])\n",
    "    \n",
    "    \n",
    "    for i in file_list:\n",
    "        df = pd.read_csv(path+'/filtered/'+i,encoding='cp949')\n",
    "        for j in range(len(df)-1,-1,-1) :\n",
    "            if df.iloc[j].loc['matching_상권_코드_명'] not in infection_area_name_list :\n",
    "                df.drop(index=j, inplace=True)\n",
    "            else : \n",
    "                pass\n",
    "        df.to_csv(path+'/filtered/filtered/'+i,encoding='cp949')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8797a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         matching_상권명                                           geometry\n",
      "0               낙성대시장  POLYGON ((126.95719 37.47880, 126.95652 37.477...\n",
      "1            봉천제일종합시장  POLYGON ((126.94472 37.48094, 126.94404 37.481...\n",
      "2                도곡시장  POLYGON ((127.05253 37.49702, 127.05192 37.497...\n",
      "3              강남개포시장  POLYGON ((127.06765 37.48935, 127.06904 37.489...\n",
      "4              화곡본동시장  POLYGON ((126.84379 37.54250, 126.84272 37.543...\n",
      "...               ...                                                ...\n",
      "1489            양재역_3  POLYGON ((127.03268 37.48735, 127.03196 37.487...\n",
      "1490            양재역_2  POLYGON ((127.03805 37.48366, 127.03748 37.483...\n",
      "1491     서울 관악구 신림역_1  POLYGON ((126.92889 37.48178, 126.92910 37.481...\n",
      "1492     서울 관악구 신림역_2  POLYGON ((126.93000 37.48128, 126.92969 37.484...\n",
      "1493  서울 서초구 남부터미널역_1  POLYGON ((127.01975 37.48412, 127.01957 37.483...\n",
      "\n",
      "[1494 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\story\\anaconda3\\envs\\geo_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "년?2020\n",
      "시작 분기?1\n",
      "끝 분기?2\n",
      "서울시 우리마을가게 상권분석서비스(상권-생활인구).csv 진입\n",
      "■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■서울시 우리마을가게 상권분석서비스(상권-생활인구).csv처리 끝\n",
      "서울시 우리마을가게 상권분석서비스(상권-아파트).csv 진입\n",
      "■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■서울시 우리마을가게 상권분석서비스(상권-아파트).csv처리 끝\n",
      "서울시 우리마을가게 상권분석서비스(상권-직장인구).csv 진입\n",
      "■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■서울시 우리마을가게 상권분석서비스(상권-직장인구).csv처리 끝\n",
      "서울시 우리마을가게 상권분석서비스(상권-집객시설).csv 진입\n",
      "■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■서울시 우리마을가게 상권분석서비스(상권-집객시설).csv처리 끝\n",
      "서울시우리마을가게상권분석서비스(상권-추정매출)_2020.csv 진입\n",
      "■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■"
     ]
    }
   ],
   "source": [
    "area = area_info(geo_path)\n",
    "        \n",
    "df_infection = area.load_infection_data()\n",
    "geo_infection = area.convert_geo_to_geodataframe(df_infection)\n",
    "geo_seoul_area = area.load_seoul_area_data()\n",
    "a = area.is_in_area(geo_seoul_area)\n",
    "\n",
    "fp = file_processing(sur_path)\n",
    "\n",
    "fp.merge_same_area_data(a)\n",
    "\n",
    "remove_uninfection_area(sur_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3d693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c07a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo_env]",
   "language": "python",
   "name": "conda-env-geo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
